{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à l'apprentissage automatique\n",
    "\n",
    "Sommaire:\n",
    "- [Chapitre I: Introduction](#chapitre-i-introduction)\n",
    "- [Chapitre II: Préparation de données](#chapitre-ii-préparation-de-données)\n",
    "- [Chapitre III: Classification naïve bayésienne](#chapitre-iii-classification-naïve-bayésienne)\n",
    "- [Chapitre IV: Machine à vecteurs de support](#chapitre-iv-machine-à-vecteurs-de-support)\n",
    "- [Chapitre V: Arbre de décision](#chapitre-v-arbre-de-décision)\n",
    "- [Chapitre VI: Régression linéaire](#chapitre-vi-régression-linéaire)\n",
    "- [Chapitre VII: Régression logistique](#chapitre-vii-régression-logistique)\n",
    "- [Chapitre VIII: Perceptron](#chapitre-viii-perceptron)\n",
    "- [Chapitre IX: Réseau de neurones artificiels](#chapitre-ix-réseau-de-neurones-artificiels)\n",
    "- [Chapitre X: Regroupement K-Means](#chapitre-x-regroupement-k-means)\n",
    "- [Chapitre XI: Auto-encodeurs (Maybe not!!)](#chapitre-XI: Auto-encodeurs)\n",
    "- [Chapitre XII: Apprentissage par renforcement](#chapitre-xii-apprentissage-par-renforcement)\n",
    "\n",
    "## Chapitre I: Introduction\n",
    "\n",
    "### I-1 Motivation\n",
    "\n",
    "- Certaines tâches sont difficiles à programmer manuellement: Reconnaissance de formes, Traduction par machine, Reconnaissance de la parole, Aide à la décision, etc.\n",
    "- Les données sont disponibles, qui peuvent être utilisé pour estimer la fonction de notre tâche.\n",
    "\n",
    "### I-2 Applications\n",
    "\n",
    "- Santé:\n",
    "  - Watson santé de IBM: https://www.ibm.com/watson/health/\n",
    "  - Projet Hanover de Microsoft: https://hanover.azurewebsites.net\n",
    "  - DeepMind santé de Google: https://deepmind.com/applied/deepmind-health/\n",
    "\n",
    "- Finance : Prévention de fraude, management de risques, prédiction des investissements, etc.\n",
    "- Domaine légal : cas de CaseText https://casetext.com\n",
    "- Traduction: Google traslate https://translate.google.com/\n",
    "- ... TO BE CONTINUED\n",
    "\n",
    "### I-3 Types des algorithmes d'apprentissage\n",
    "\n",
    "#### I-3-1 Apprentissage supervisé\n",
    "\n",
    "##### Classification\n",
    "\n",
    "##### Régression\n",
    "\n",
    "#### I-3-2 Apprentissage non supervisé\n",
    "\n",
    "##### Clustering (Regroupement)\n",
    "\n",
    "##### Réduction de dimension\n",
    "\n",
    "#### I-3-3 Apprentissage par renforcement  \n",
    "\n",
    "### I-4 Limites de l'apprentissage automatique\n",
    "\n",
    "### I-5 Outils de l'apprentissage automatique\n",
    "\n",
    "\n",
    "## Chapitre II: Préparation de données\n",
    "\n",
    "### II-1\n",
    "\n",
    "### II-2\n",
    "\n",
    "### Bibliographie\n",
    "\n",
    "- https://developers.google.com/machine-learning/data-prep/\n",
    "- https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/\n",
    "- https://www.altexsoft.com/blog/datascience/preparing-your-dataset-for-machine-learning-8-basic-techniques-that-make-your-data-better/\n",
    "\n",
    "\n",
    "## Chapitre III: Classification naïve bayésienne\n",
    "\n",
    "### III-1 Classification\n",
    "\n",
    "Voici la théorème de Bayes:\n",
    "\n",
    "$ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $\n",
    "\n",
    "Le problème de classification revient à estimer la probabilité de chaque classe $c_i$ en se basant sur un vecteur de critère $\\overrightarrow{f}$. \n",
    "Par example, nous voulons estimer la probabilité d'un animal étant un chien, un chat, une vache ou autre (4 classes) en se basant sur un vecteur de critères: poids, longeur, longueur des pattes et le type de nouriture.\n",
    "En appliquant la théorème de Bayes:\n",
    "\n",
    "$ P(c_i|\\overrightarrow{f}) = \\frac{P(\\overrightarrow{f}|c_i) P(c_i)}{P(\\overrightarrow{f})} $\n",
    "\n",
    "Le dénominateur ne dépend pas de la classe $c_i$\n",
    "\n",
    "$ P(c_i|\\overrightarrow{f}) \\propto P(\\overrightarrow{f}|c_i) P(c_i) $\n",
    "\n",
    "En supposant l'indépedance entre les critères $f_j$ de $\\overrightarrow{f}$ :\n",
    "\n",
    "$ P(c_i|\\overrightarrow{f}) \\propto P(c_i) \\prod\\limits_{f_j \\in \\overrightarrow{f}} P(f_j|c_i) $\n",
    "\n",
    "Les probabilités calculées servent à sélectionner la classe la plus probable sachant notre vecteur de critères. \n",
    "Donc, la classe estimée $c$ est celle qui maximize la probabilité conditionnelle. \n",
    "\n",
    "$ c  =  \\arg\\max\\limits_{ci} P(c_i|\\overrightarrow{f}) $\n",
    "\n",
    "$ c =  \\arg\\max\\limits_{ci} P(c_i) \\prod\\limits_{f_j \\in \\overrightarrow{f}} P(f_j|c_i) $\n",
    "\n",
    "\n",
    "### III-2 Apprentissage (Estimation des paramètres du modèle)\n",
    "\n",
    "Etant donné un ensemble de données, la probabilité d'apparaition d'une classe $c_i$ est estimée comme le nombre des exemplaires de $c_i$ sur le nombre totale des examplaires dans cette ensemble.\n",
    "\n",
    "$ P(c_i) = \\frac{|c_i|}{\\sum_{c_j} |c_j|} $\n",
    "\n",
    "La probabilité de chaque critère $f_j$ sachant une classe $c_i$ est estimée selon le type de ces valeurs: discrètes, binaires ou continues.\n",
    "\n",
    "#### Loi multinomiale\n",
    "\n",
    "Si les valeurs de notre critère sont discrètes, on utilise la loi multinomiale. \n",
    "Par exemple, la couleur des cheveux avec les valeurs: brun, auburn, châtain, roux, blond vénitien, blond et blanc.\n",
    "La probabilité d'un critère $f_j$ sachant une classe $c_i$ est le nombre des occurrences de ce critère dans la classe ($|c_i|_{f_j}$) divisé par le nombre de ces occurrences dans tout l'ensemble de données.\n",
    "\n",
    "$ P(f_j|c_i) = \\frac{|c_i|_{f_j}}{\\sum_{c_j} |c_j|_{f_j}} $\n",
    "\n",
    "\n",
    "#### Loi de Bernoulli\n",
    "caractéristiques binaires\n",
    "\n",
    "#### Loi normal\n",
    "valeurs continues\n",
    "\n",
    "### III-3 Application (Points forts)\n",
    "\n",
    "### III-4 Limites\n",
    "\n",
    "### Bibliographie\n",
    "\n",
    "- https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67\n",
    "- https://syncedreview.com/2017/07/17/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation/\n",
    "- https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n",
    "- https://www.geeksforgeeks.org/naive-bayes-classifiers/\n",
    "- https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c\n",
    "\n",
    "## Chapitre IV: Machine à vecteurs de support\n",
    "\n",
    "## Chapitre V: Machine à vecteurs de support\n",
    "\n",
    "## Chapitre VI: Régression linéaire\n",
    "\n",
    "## Chapitre VII: Régression logistique\n",
    "\n",
    "## Chapitre VIII: Perceptron\n",
    "\n",
    "## Chapitre IX: Réseau de neurones artificiels\n",
    "\n",
    "## Chapitre X: Regroupement\n",
    "\n",
    "## X-1 Regroupement hiérarchique\n",
    "\n",
    "## X-2 K-Means\n",
    "\n",
    "### Bibliographie\n",
    "\n",
    "- https://towardsdatascience.com/unsupervised-learning-with-python-173c51dc7f03\n",
    "\n",
    "## Chapitre XI: Auto-encodeurs (Maybe not!!)\n",
    "\n",
    "## Chapitre XII: Apprentissage par renforcement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
